[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "linearregression.html",
    "href": "linearregression.html",
    "title": "linear regression",
    "section": "",
    "text": "What is Linear Regression?\nLinear regression predicts the relationship between two variables by assuming they have a straight-line connection. It finds the best line that minimizes the differences between predicted and actual values. Used in fields like economics and finance, it helps analyze and forecast data trends. Linear regression can also involve several variables (multiple linear regression) or be adapted for yes/no questions (logistic regression).\nSimple Linear Regression\nIn a simple linear regression, there is one independent variable and one dependent variable. The model estimates the slope and intercept of the line of best fit, which represents the relationship between the variables. The slope represents the change in the dependent variable for each unit change in the independent variable, while the intercept represents the predicted value of the dependent variable when the independent variable is zero.\nLinear regression is a quiet and the simplest statistical regression technique used for predictive analysis in machine learning. Linear regression shows the linear relationship between the independent(predictor) variable i.e. X-axis and the dependent (output) variable i.e. Y-axis, called linear regression. If there is a single input variable X (independent variable), such linear regression is simple linear regression.\nSimple Linear Regression The graph above presents the linear relationship between the output(y) and predictor(X) variables. The blue line is referred to as the best-fit straight line. Based on the given data points, we attempt to plot a line that fits the points the best.\nSimple Regression Calculation\nTo calculate best-fit line linear regression uses a traditional slope-intercept form which is given below,\nY i = β 0 + β 1 X i\nwhere Y i = Dependent variable, β 0 = constant/Intercept, β 1 = Slope/Intercept, X i = Independent variable.\nThis algorithm explains the linear relationship between the dependent(output) variable y and the independent(predictor) variable X using a straight line Y= B 0 + B 1 X.\nBut how does the linear regression find out which is the best-fit line?\nThe goal of the linear regression algorithm is to get the best values for B 0 and B 1 to find the best-fit line. The best-fit line is a line that has the least error which means the error between predicted values and actual values should be minimum.\nSimple Linear Regression explanation But how the linear regression finds out which is the best fit line?\nThe goal of the linear regression algorithm is to get the best values for B0 and B1 to find the best fit line. The best fit line is a line that has the least error which means the error between predicted values and actual values should be minimum.\nRandom Error(Residuals)\nIn regression, the difference between the observed value of the dependent variable(y i ) and the predicted value(predicted) is called the residuals.\nε i = y predicted – y i\nwhere y predicted = B 0 + B 1 X i\nWhat is the Best Fit Line?\nIn simple terms, the best-fit line is a line that best fits the given scatter plot. Mathematically, you obtain the best-fit line by minimizing the Residual Sum of Squares (RSS).\nCost Function for Linear Regression\nThe cost function helps to work out the optimal values for B 0 and B 1 , which provides the best-fit line for the data points.\nIn Linear Regression, generally Mean Squared Error (MSE) cost function is used, which is the average squared error that occurred between the y predicted and y i.\nWe calculate MSE using the simple linear equation y=mx+b:\nMSE Using the MSE function, we’ll update the values of B 0 and B 1 such that the MSE value settles at the minima. These parameters can be determined using the gradient descent method such that the value for the cost function is minimum.\nGradient Descent for Linear Regression\nGradient Descent is one of the optimization algorithms that optimize the cost function (objective function) to reach the optimal minimal solution. To find the optimum solution, we need to reduce the cost function (MSE) for all data points. This is done by updating the values of the slope coefficient (B1) and the constant coefficient (B0) iteratively until we get an optimal solution for the linear function.\nA regression model optimizes the gradient descent algorithm to update the coefficients of the line by reducing the cost function by randomly selecting coefficient values and then iteratively updating the coefficient values to reach the minimum cost function.\nGradient Descent for Linear Regression Gradient Descent Example\nLet’s take an example to understand this. Imagine a U-shaped pit. You are standing at the uppermost point in the pit, and your motive is to reach the bottom of the pit. Suppose there is a treasure at the bottom of the pit, and you can only take a discrete number of steps to reach the bottom. If you opted to take one step at a time, you would get to the bottom of the pit in the end but, this would take a longer time. If you decide to take larger steps each time, you may achieve the bottom sooner but, there’s a probability that you could overshoot the bottom of the pit and not even near the bottom. In the gradient descent algorithm, the number of steps you’re taking can be considered as the learning rate, and this decides how fast the algorithm converges to the minima.\nGradient Descent for Linear Regression | minima and maxima To update B 0 and B 1, we take gradients from the cost function. To find these gradients, we take partial derivatives for B 0 and B 1.\nGradient Descent for Linear Regression | formula We need to minimize the cost function J. One of the ways to achieve this is to apply the batch gradient descent algorithm. In batch gradient descent, the values are updated in each iteration. (The last two equations show the updating of values)\nThe partial derivates are the gradients, and they are used to update the values of B 0 and B 1. Alpha is the learning rate.\nWhy Linear Regression is Important?\nLinear regression is important for a few reasons:\nSimplicity and interpretability: It’s a relatively easy concept to understand and apply. The resulting simple linear regression model is a straightforward equation that shows how one variable affects another. This makes it easier to explain and trust the results compared to more complex models. Prediction: Linear regression allows you to predict future values based on existing data. For instance, you can use it to predict sales based on marketing spend or house prices based on square footage. Foundation for other techniques: It serves as a building block for many other data science and machine learning methods. Even complex algorithms often rely on linear regression as a starting point or for comparison purposes. Widespread applicability: Linear regression can be used in various fields, from finance and economics to science and social sciences. It’s a versatile tool for uncovering relationships between variables in many real-world scenarios. In essence, linear regression provides a solid foundation for understanding data and making predictions. It’s a cornerstone technique that paves the way for more advanced data analysis methods.\nEvaluation Metrics for Linear Regression\nThe strength of any linear regression model can be assessed using various evaluation metrics. These evaluation metrics usually provide a measure of how well the observed outputs are being generated by the model.\nThe most used metrics are,\nCoefficient of Determination or R-squared (R2) Root Mean Squared Error (RSME) and Residual Standard Error (RSE) Coefficient of Determination or R-Squared (R2)\nR-squared is a number that explains the amount of variation that is explained/captured by the developed model. It always ranges between 0 & 1 . Overall, the higher the value of R-squared, the better the model fits the data.\nMathematically it can be represented as,\nR2 = 1 – ( RSS/TSS )\nResidual sum of Squares (RSS) is defined as the sum of squares of the residual for each data point in the plot/data. It is the measure of the difference between the expected and the actual observed output. Residual Sum of Squares | Linear regression Total Sum of Squares (TSS) is defined as the sum of errors of the data points from the mean of the response variable. Mathematically TSS is, Total Sum of Squares where y hat is the mean of the sample data points.\nThe significance of R-squared is shown by the following figures,\nr-squared significance Root Mean Squared Error\nThe Root Mean Squared Error is the square root of the variance of the residuals. It specifies the absolute fit of the model to the data i.e. how close the observed data points are to the predicted values. Mathematically it can be represented as,\nRoot Mean Squared Error To make this estimate unbiased, one has to divide the sum of the squared residuals by the degrees of freedom rather than the total number of data points in the model. This term is then called the Residual Standard Error(RSE). Mathematically it can be represented as,\nResidual Standard Error R-squared is a better measure than RSME. Because the value of Root Mean Squared Error depends on the units of the variables (i.e. it is not a normalized measure), it can change with the change in the unit of the variables.\nAssumptions of Linear Regression\nRegression is a parametric approach, which means that it makes assumptions about the data for analysis. For successful regression analysis, it’s essential to validate the following assumptions.\nLinearity of residuals: There needs to be a linear relationship between the dependent variable and independent variable(s). Linearity of residuals Independence of residuals: The error terms should not be dependent on one another (like in time-series data wherein the next value is dependent on the previous one). There should be no correlation between the residual terms. The absence of this phenomenon is known as Autocorrelation. There should not be any visible patterns in the error terms.\nIndependence of residuals Normal distribution of residuals: The mean of residuals should follow a normal distribution with a mean equal to zero or close to zero. This is done to check whether the selected line is the line of best fit or not. If the error terms are non-normally distributed, suggests that there are a few unusual data points that must be studied closely to make a better model. Normal distribution of residual 4. The equal variance of residuals: The error terms must have constant variance. This phenomenon is known as Homoscedasticity. The presence of non-constant variance in the error terms is referred to as Heteroscedasticity. Generally, non-constant variance arises in the presence of outliers or extreme leverage values.\nThe equal variance of residuals Hypothesis in Linear Regression\nOnce you have fitted a straight line on the data, you need to ask, “Is this straight line a significant fit for the data?” Or “Is the beta coefficient explain the variance in the data plotted?” And here comes the idea of hypothesis testing on the beta coefficient. The Null and Alternate hypotheses in this case are:\nH 0 : B 1 = 0\nH A : B 1 ≠ 0\nTo test this hypothesis we use a t-test, test statistics for the beta coefficient is given by,\nt-test for linear regression Assessing the Model Fit\nSome other parameters to assess a model are:\nt statistic: It is used to determine the p-value and hence, helps in determining whether the coefficient is significant or not F statistic: It is used to assess whether the overall model fit is significant or not. Generally, the higher the value of the F-statistic, the more significant a model turns out to be. Multiple Linear Regression\nMultiple linear regression is a technique to understand the relationship between a single dependent variable and multiple independent variables.\nThe formulation for multiple linear regression is also similar to simple linear regression with\nthe small change that instead of having one beta variable, you will now have betas for all the variables used. The formula is given as:\nY = B0 + B1X1 + B2X2 + … + BpXp + ε\nConsiderations of Multiple Linear Regression\nAll four assumptions made for Simple Linear Regression still hold for Multiple Linear Regression along with a few new additional assumptions.\nOverfitting: When more and more variables are added to a model, the model may become far too complex and usually ends up memorizing all the data points in the training set. This phenomenon is known as the overfitting of a model. This usually leads to high training accuracy and very low test accuracy. Multicollinearity: It is the phenomenon where a model with several independent variables, may have some variables interrelated. Feature Selection: With more variables present, selecting the optimal set of predictors from the pool of given features (many of which might be redundant) becomes an important task for building a relevant and better model. Multicollinearity\nAs multicollinearity makes it difficult to find out which variable is contributing towards the prediction of the response variable, it leads one to conclude incorrectly, the effects of a variable on the target variable. Though it does not affect the precision of the model predictions, it is essential to properly detect and deal with the multicollinearity present in the model, as random removal of any of these correlated variables from the model causes the coefficient values to swing wildly and even change signs.\nMulticollinearity can be detected using the following methods.\nPairwise Correlations: Checking the pairwise correlations between different pairs of independent variables can throw useful insights into detecting multicollinearity. Variance Inflation Factor (VIF): Pairwise correlations may not always be useful as it is possible that just one variable might not be able to completely explain some other variable but some of the variables combined could be ready to do this. Thus, to check these sorts of relations between variables, one can use VIF. VIF explains the relationship of one independent variable with all the other independent variables. VIF is given by, multicollinearity VIF where i refers to the ith variable which is being represented as a linear combination of the rest of the independent variables.\nThe common heuristic followed for the VIF values is if VIF &gt; 10 then the value is high and it should be dropped. And if the VIF=5 then it may be valid but should be inspected first. If VIF &lt; 5, then it is considered a good VIF value.\nOverfitting and Underfitting in Linear Regression\nThere have always been situations where a model performs well on training data but not on the test data. While training models on a dataset, overfitting, and underfitting are the most common problems faced by people.\nBefore understanding overfitting and underfitting one must know about bias and variance.\nBias\nBias is a measure to determine how accurate a model’s predictions are likely to be on future unseen data. Complex models, assuming there is enough training data available, can make accurate model predictions. Whereas the models that are too naive, are very likely to perform badly concerning model predictions. Simply, Bias is errors made by training data.\nGenerally, linear algorithms have a high bias which makes them fast to learn and easier to understand but in general, are less flexible. Implying lower predictive performance on complex problems that fail to meet the expected outcomes.\nVariance\nVariance is the sensitivity of the model towards training data, that is it quantifies how much the model will react when input data is changed.\nIdeally, the model shouldn’t change too much from one training dataset to the next training data, which means that the algorithm is good at picking out the hidden underlying patterns between the inputs and the output variables.\nIdeally, a model should have lower variance which means that the model doesn’t change drastically after changing the training data(it is generalizable). Having higher variance will make a model change drastically even on a small change in the training dataset.\nLet’s understand what a bias-variance tradeoff is.\nBias Variance Tradeoff\nbias and variance tradeoff for linear regression In the pursuit of optimal performance, a supervised machine learning algorithm seeks to strike a balance between low bias and low variance for increased robustness.\nIn the realm of machine learning, there exists an inherent relationship between bias and variance, characterized by an inverse correlation.\nIncreased bias leads to reduced variance. Conversely, heightened variance results in diminished bias. Finding an equilibrium between bias and variance is crucial, and algorithms must navigate this trade-off for optimal outcomes.\nIn practice, calculating precise bias and variance error terms is challenging due to the unknown nature of the underlying target function.\nNow, let’s delve into the nuances of overfitting and underfitting.\nOverfitting\nWhen a model learns every pattern and noise in the data to such an extent that it affects the performance of the model on the unseen future dataset, it is referred to as overfitting. The model fits the data so well that it interprets noise as patterns in the data.\nWhen a model has low bias and higher variance it ends up memorizing the data and causing overfitting. Overfitting causes the model to become specific rather than generic. This usually leads to high training accuracy and very low test accuracy.\nDetecting overfitting is useful, but it doesn’t solve the actual problem. There are several ways to prevent overfitting, which are stated below:\nCross-validation If the training data is too small to train add more relevant and clean data. If the training data is too large, do some feature selection and remove unnecessary features. Regularization Underfitting\nUnderfitting is not often discussed as often as overfitting is discussed. When the model fails to learn from the training dataset and is also not able to generalize the test dataset, is referred to as underfitting. This type of problem can be very easily detected by the performance metrics.\nWhen a model has high bias and low variance it ends up not generalizing the data and causing underfitting. It is unable to find the hidden underlying patterns in the data. This usually leads to low training accuracy and very low test accuracy. The ways to prevent underfitting are stated below,\nIncrease the model complexity Increase the number of features in the training data Remove noise from the data.",
    "crumbs": [
      "linear regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "akmsquarto",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout.",
    "crumbs": [
      "Quartoinformation"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "akmsquarto",
    "section": "",
    "text": "It’s possible to create beautiful documentation to share online with Quarto that auto-updates with GitHub. This is very new and incredibly cool. This tutorial is an example of a quarto website — it is a really powerful way to create and share your work. You can communicate about science using the same reproducible workflow you and/or your colleagues use for analyses, whether or not you write code.\nCreating websites with Quarto can be done without knowing R, Python or HTML, CSS, etc, and that’s where we’ll start. However, Quarto integrates with these tools so you can make your websites as complex and beautiful as you like as you see examples and reuse and remix from others in the open community. This tutorial borrows heavily from a lot of great tutorials and resources you should check out too – there are links throughout.",
    "crumbs": [
      "Quartoinformation"
    ]
  },
  {
    "objectID": "index.html#what-is-quarto",
    "href": "index.html#what-is-quarto",
    "title": "akmsquarto",
    "section": "What is Quarto?",
    "text": "What is Quarto?\nQuarto helps you have your ideas and your code in one place, and present it in a beautiful way.\nQuarto unifies and extends the RMarkdown ecosystem - it unifies by combining the functionality of R Markdown, bookdown, distill, xaringian, etc into a single consistent system. And it extends in several ways: all features are possible beyond R too, including Python and Javascript. It also has more “guardrails”: accessibility and inclusion are centered in the design. Quarto is for people who love RMarkdown, and it’s for people who have never used RMarkdown.\nThe ability for Quarto to streamline collaboration has been so cool and important for our NASA Openscapes project. Quarto has been a common place for us to collaborate - across R and Python languages and coding expertise.",
    "crumbs": [
      "Quartoinformation"
    ]
  },
  {
    "objectID": "index.html#what-is-this-tutorial",
    "href": "index.html#what-is-this-tutorial",
    "title": "akmsquarto",
    "section": "What is this tutorial?",
    "text": "What is this tutorial?\nThis is a 1-hour tutorial that can be used to teach or as self-paced learning.\nWe introduce Quarto by exploring this tutorial website, and practicing the basic Quarto workflow using different tools (GitHub browser, RStudio, and Jupyter) for editing your website.\nWe’ll start off from the browser so you don’t need to install any additional software, however this approach is very limited and you will soon outgrow its capabilities. If you don’t already have a workflow to edit files and sync to GitHub from your computer, I recommend RStudio. You don’t need to know R to use RStudio, and it has powerful editor features that make for happy workflows.\nQuarto.org is the go-to place for full documentation and more tutorials!\n\n1 + 1\n\n[1] 2\n\n\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00  \n\n\n\n\nsummary(iris)\n\n  Sepal.Length    Sepal.Width     Petal.Length    Petal.Width   \n Min.   :4.300   Min.   :2.000   Min.   :1.000   Min.   :0.100  \n 1st Qu.:5.100   1st Qu.:2.800   1st Qu.:1.600   1st Qu.:0.300  \n Median :5.800   Median :3.000   Median :4.350   Median :1.300  \n Mean   :5.843   Mean   :3.057   Mean   :3.758   Mean   :1.199  \n 3rd Qu.:6.400   3rd Qu.:3.300   3rd Qu.:5.100   3rd Qu.:1.800  \n Max.   :7.900   Max.   :4.400   Max.   :6.900   Max.   :2.500  \n       Species  \n setosa    :50  \n versicolor:50  \n virginica :50",
    "crumbs": [
      "Quartoinformation"
    ]
  }
]